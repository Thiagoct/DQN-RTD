{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import robot_goal\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = robot_goal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'env.reset()\\n\\ndone = False\\ntotal_reward = 0\\nsimulation_start = 0\\nwhile simulation_start != 1:\\n    simulation_start = env.sim.startSimulation()\\nwhile not done:\\n   action = random.randint(0,2)\\n   obs, rew, done = env.step(action)\\n   total_reward += rew\\n   print(f\"{obs} -> {rew}\")\\nsimulation_end = 0        \\nwhile simulation_end != 1:\\n    simulation_end = env.sim.stopSimulation()     \\nprint(f\"Total reward: {total_reward}\")'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "simulation_start = 0\n",
    "while simulation_start != 1:\n",
    "    simulation_start = env.sim.startSimulation()\n",
    "while not done:\n",
    "   action = random.randint(0,2)\n",
    "   obs, rew, done = env.step(action)\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "simulation_end = 0        \n",
    "while simulation_end != 1:\n",
    "    simulation_end = env.sim.stopSimulation()     \n",
    "print(f\"Total reward: {total_reward}\")\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 10\n",
    "num_actions = 3\n",
    "\n",
    "num_hidden = 128\n",
    "\n",
    "inputs = keras.layers.Input(shape=(num_inputs,))\n",
    "common = keras.layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
    "action = keras.layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "critic = keras.layers.Dense(1)(common)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 10)]                 0         []                            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 128)                  1408      ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 3)                    387       ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 1)                    129       ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1924 (7.52 KB)\n",
      "Trainable params: 1924 (7.52 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_episode(max_steps_per_episode=10000, render=False):\n",
    "    states, actions, probs, rewards, critic = [], [], [], [], []  # Inicialização de listas para armazenar informações durante o episódio\n",
    "    state = env.reset()  # Reinicialização do ambiente e obtenção do estado inicial\n",
    "    simulation_start = 0\n",
    "    while simulation_start != 1:\n",
    "        simulation_start = env.sim.startSimulation()  # Inicialização da simulação\n",
    "        \n",
    "    for _ in tqdm(range(max_steps_per_episode)):  # Loop para cada etapa do episódio\n",
    "        state = np.reshape(state, (1, state.shape[0]))  # Reformatação do estado para atender aos requisitos do modelo\n",
    "        action_probs, est_rew = model(state)  # Obtenção das probabilidades de ação e da estimativa de recompensa do modelo\n",
    "        action = np.argmax(action_probs)  # Escolha da ação com a maior probabilidade\n",
    "        nstate, reward, done = env.step(action)  # Execução da ação no ambiente e obtenção do próximo estado, recompensa e indicador de conclusão\n",
    "        \n",
    "        if done:\n",
    "            break  # Se o episódio estiver concluído, encerra o loop\n",
    "        \n",
    "        # Armazenamento das informações do episódio\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(tf.math.log(action_probs[0, action]))\n",
    "        rewards.append(reward)\n",
    "        critic.append(est_rew[0, 0])\n",
    "        \n",
    "        state = nstate  # Atualização do estado para o próximo estado\n",
    "        \n",
    "    return states, actions, probs, rewards, critic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 4077/10000 [41:35<1:18:59,  1.25it/s]"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "huber_loss = keras.losses.Huber()\n",
    "episode_count = 0\n",
    "running_reward = 0\n",
    "\n",
    "while True:\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        _,_,action_probs, rewards, critic_values = run_episode()\n",
    "        episode_reward = np.sum(rewards)\n",
    "\n",
    "        # Atualizar recompensa em execução para verificar condição de resolução.\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # Calculando valores de perda para atualizar nossa rede.\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, value, rew in zip(action_probs, critic_values, rewards):\n",
    "            # Quando tomamos a ação com probabilidade log_prob, recebemos uma\n",
    "            # recompensa descontada de rew, enquanto o crítico a previu como value. \n",
    "            # Primeiro, calculamos a perda do ator, para fazer com que o ator preveja ações que levem a recompensas mais altas.\n",
    "            diff = rew - value\n",
    "            actor_losses.append(-log_prob * diff)\n",
    "\n",
    "            # A perda do crítico visa minimizar a diferença entre a recompensa prevista 'value' e a recompensa descontada real 'rew'.\n",
    "            critic_losses.append(\n",
    "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(rew, 0))\n",
    "            )\n",
    "\n",
    "        # Backpropagation\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Logs\n",
    "    episode_count += 1\n",
    "    if episode_count % 10 == 0:\n",
    "        model.save(\"model.h5\")\n",
    "        template = \"running reward: {:.2f} at episode {}\"\n",
    "        print(template.format(running_reward, episode_count))\n",
    "\n",
    "    if running_reward > 195:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
