{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import robot_goal\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = robot_goal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'env.reset()\\n\\ndone = False\\ntotal_reward = 0\\nsimulation_start = 0\\nwhile simulation_start != 1:\\n    simulation_start = env.sim.startSimulation()\\nwhile not done:\\n   action = random.randint(0,2)\\n   obs, rew, done = env.step(action)\\n   total_reward += rew\\n   print(f\"{obs} -> {rew}\")\\nsimulation_end = 0        \\nwhile simulation_end != 1:\\n    simulation_end = env.sim.stopSimulation()     \\nprint(f\"Total reward: {total_reward}\")'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "simulation_start = 0\n",
    "while simulation_start != 1:\n",
    "    simulation_start = env.sim.startSimulation()\n",
    "while not done:\n",
    "   action = random.randint(0,2)\n",
    "   obs, rew, done = env.step(action)\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "simulation_end = 0        \n",
    "while simulation_end != 1:\n",
    "    simulation_end = env.sim.stopSimulation()     \n",
    "print(f\"Total reward: {total_reward}\")\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 10\n",
    "num_actions = 3\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation=\"relu\",input_shape=(num_inputs,)),\n",
    "    keras.layers.Dense(num_actions, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               1408      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1795 (7.01 KB)\n",
      "Trainable params: 1795 (7.01 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 100):\n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    simulation_start = 0\n",
    "    while simulation_start != 1:\n",
    "        simulation_start = env.sim.startSimulation()\n",
    "    for i in tqdm(range(max_steps_per_episode)):\n",
    "        state = np.reshape(state,(1, state.shape[0]))\n",
    "        action_probs = model.predict(state, verbose=0)\n",
    "        action = np.argmax(action_probs)\n",
    "        nstate, reward, done = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs)\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    simulation_end = 0        \n",
    "    while simulation_end != 1:\n",
    "        simulation_end = env.sim.stopSimulation()     \n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:26<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 19.999999999999996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "s,a, p, r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.18060876, -0.04000501,  0.0268635 , ...,  0.15091164,\n",
       "         -0.12753296,  0.00915667],\n",
       "        [ 0.16119222,  0.1624736 , -0.1878345 , ...,  0.14091556,\n",
       "         -0.04019155, -0.17210284],\n",
       "        [ 0.0523188 ,  0.11685924,  0.06878991, ..., -0.05535288,\n",
       "         -0.06048627,  0.05600075],\n",
       "        ...,\n",
       "        [ 0.02262938,  0.20555405,  0.15970235, ..., -0.02016139,\n",
       "          0.13872905, -0.10778882],\n",
       "        [ 0.15697779,  0.05459012, -0.02920412, ...,  0.1748731 ,\n",
       "          0.14470752, -0.14837846],\n",
       "        [ 0.11075254,  0.1204754 ,  0.03532745, ...,  0.00410755,\n",
       "          0.1186709 ,  0.10154273]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:30<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> 19.999999999999996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:06<03:26,  2.13s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\thiag\\OneDrive\\Documentos\\GitHub\\DQN-RTD\\atividade4.ipynb Cell 11\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thiag/OneDrive/Documentos/GitHub/DQN-RTD/atividade4.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m history \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thiag/OneDrive/Documentos/GitHub/DQN-RTD/atividade4.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m300\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/thiag/OneDrive/Documentos/GitHub/DQN-RTD/atividade4.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     states, actions, probs, rewards \u001b[39m=\u001b[39m run_episode()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thiag/OneDrive/Documentos/GitHub/DQN-RTD/atividade4.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     one_hot_actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39meye(\u001b[39m3\u001b[39m)[actions\u001b[39m.\u001b[39mT][\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thiag/OneDrive/Documentos/GitHub/DQN-RTD/atividade4.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     gradients \u001b[39m=\u001b[39m one_hot_actions\u001b[39m-\u001b[39mprobs\n",
      "\u001b[1;32mc:\\Users\\thiag\\OneDrive\\Documentos\\GitHub\\DQN-RTD\\atividade4.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thiag/OneDrive/Documentos/GitHub/DQN-RTD/atividade4.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m action_probs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(state, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thiag/OneDrive/Documentos/GitHub/DQN-RTD/atividade4.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(action_probs)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/thiag/OneDrive/Documentos/GitHub/DQN-RTD/atividade4.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m nstate, reward, done \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thiag/OneDrive/Documentos/GitHub/DQN-RTD/atividade4.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mif\u001b[39;00m done:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thiag/OneDrive/Documentos/GitHub/DQN-RTD/atividade4.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thiag\\OneDrive\\Documentos\\GitHub\\DQN-RTD\\env.py:163\u001b[0m, in \u001b[0;36mrobot_goal.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid]\u001b[39m=\u001b[39mts\n\u001b[0;32m    162\u001b[0m \u001b[39m# Criação de estado e cálculo da ação de controle\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m observation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__state_creator()\n\u001b[0;32m    164\u001b[0m \u001b[39m# Cálculo de velocidades linear e angular\u001b[39;00m\n\u001b[0;32m    165\u001b[0m v, omega \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__getLyapunov(\u001b[39m0.9\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__conversor_alpha(action), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtheta_global)\n",
      "File \u001b[1;32mc:\\Users\\thiag\\OneDrive\\Documentos\\GitHub\\DQN-RTD\\env.py:81\u001b[0m, in \u001b[0;36mrobot_goal.__state_creator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__state_creator\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     80\u001b[0m     \u001b[39m# Obtenção de posições e valores dos sensores\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__getSensorvalues_and_positions()\n\u001b[0;32m     82\u001b[0m     \u001b[39m# Cálculo de e, alpha e theta\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__getErros()\n",
      "File \u001b[1;32mc:\\Users\\thiag\\OneDrive\\Documentos\\GitHub\\DQN-RTD\\env.py:54\u001b[0m, in \u001b[0;36mrobot_goal.__getSensorvalues_and_positions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39multrasonicSensor_value \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39m16\u001b[39m, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m16\u001b[39m):\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39multrasonicSensor_value[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msim\u001b[39m.\u001b[39;49mreadProximitySensor(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49multrasonicSensor_list[i],\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m     55\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mxp[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid]\u001b[39m=\u001b[39mP3DXPos[\u001b[39m0\u001b[39m]\n\u001b[0;32m     56\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39myp[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid]\u001b[39m=\u001b[39mP3DXPos[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\coppeliasim_zmqremoteapi_client\\__init__.py:82\u001b[0m, in \u001b[0;36mRemoteAPIClient.getObject.<locals>.<lambda>\u001b[1;34m(func, *a)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mfound nondict\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     81\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(v) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mfunc\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m v:\n\u001b[1;32m---> 82\u001b[0m     \u001b[39msetattr\u001b[39m(ret, k, \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39ma, func\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall(func, a))\n\u001b[0;32m     83\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(v) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mconst\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m v:\n\u001b[0;32m     84\u001b[0m     \u001b[39msetattr\u001b[39m(ret, k, v[\u001b[39m'\u001b[39m\u001b[39mconst\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\coppeliasim_zmqremoteapi_client\\__init__.py:71\u001b[0m, in \u001b[0;36mRemoteAPIClient.call\u001b[1;34m(self, func, args)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call function with specified arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send({\u001b[39m'\u001b[39m\u001b[39mfunc\u001b[39m\u001b[39m'\u001b[39m: func, \u001b[39m'\u001b[39m\u001b[39margs\u001b[39m\u001b[39m'\u001b[39m: args})\n\u001b[1;32m---> 71\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\coppeliasim_zmqremoteapi_client\\__init__.py:51\u001b[0m, in \u001b[0;36mRemoteAPIClient._recv\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_recv\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> 51\u001b[0m     rawResp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket\u001b[39m.\u001b[39;49mrecv()\n\u001b[0;32m     52\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     53\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mReceived raw len=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(rawResp)\u001b[39m}\u001b[39;00m\u001b[39m, base64=\u001b[39m\u001b[39m{\u001b[39;00mb64(rawResp)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mzmq\\\\backend\\\\cython\\\\socket.pyx:805\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mzmq\\\\backend\\\\cython\\\\socket.pyx:841\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mzmq\\\\backend\\\\cython\\\\socket.pyx:194\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\zmq\\backend\\cython\\checkrc.pxd:13\u001b[0m, in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(3)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    model.train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
